   \documentclass[openany]{book} % oppure \openany in mezzo al libro
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage[T1]{fontenc}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{geometry}
\usepackage{amsfonts}
\usepackage{multicol}
\usepackage[none]{hyphenat}
\usepackage{blindtext}
\usepackage{graphicx}


\sloppy
%\raggedright 
 \geometry{
 a4paper,
 left=20mm,
 right=20mm,
 top=20mm,
 bottom=20mm,
}

\newcommand{\ind}{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}

\pagestyle{empty}

\title{Titolo}
\date{}
\author{MT}
\renewcommand{\baselinestretch}{1.2}

\setlength{\parindent}{0em}
%\setlength{\parskip}{0.2em}

\begin{document}

\begin{center}
\Large Probabilità e Statistica 

\large Formulario
\end{center}

$e^{ix} = \cos(x)+i\sin(x)$

\textbf{Disposizioni con ripetizione} $D^R_{n.k}=n^k$

\textbf{Disposizioni semplici} $D_{n,k}=\frac{n!}{(n-k)!}$

\textbf{Permutazioni di n elementi} \quad $P_n = D_{n,n}=n!$

\textbf{Combinazioni semplici} (numero di sottoinsiemi di cardinalità $k$) \quad $C_{n,k}=\binom {n}{k} = \frac{n!}{k!(n-k)!}$

\textbf{Coefficiente multinomiale} (numero di modi in cui si può partizionare un insieme in $r$ sottoinsiemi di cardinalità $k_1,\dots,k_n$) \quad $\binom {n}{k_1,\dots,k_n}=\frac{n!}{k_1 ! \dots k_n !}$ 

\textbf{Formula di Poincaré} (principio di inclusione - esclusione) \quad $\mu(A_1 \cup \dots \cup A_n) =$ 

$= \sum_{k=1}^n (-1)^{k+1}\sum_{1\leq i_1 < \dots <i_k \leq n}\mu(A_{i_1}\cap \dots \cap A_{i_k})$, $\quad \mu$ misura finita

\textbf{Valore atteso} $\mathbb{E}(X) = \int_{-\infty}^{+\infty}xf(x)\,dx$

\textbf{Varianza} $\text{Var}(X)=\mathbb{E}(X^2)-\mathbb{E}(X)^2$

\textbf{Covarianza} $\text{Cov}(X,Y)=\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)$

\textbf{Coefficiente di correlazione} $\rho_{XY} = \frac{\text{Cov}(X,Y)}{\sqrt {\text{Var}(X)\text{Var}(X)}}$

\textbf{Algebra dei valori attesi}

$\mathbb {E}(aX+bY+c)= a\mathbb{E}(X)+b \mathbb{E}(Y)+c$

$\text {Var}(aX+bY+c)= a^2 \text{Var}(X)+b^2 \text{Var}(Y)+2ab\text{Cov}(X,Y)$

$\text{Cov}(aX+bY+c,Z) = a \text{Cov}(X,Z)+b \text{Cov(Y,Z)}$

\textbf{Convoluzione} $X,\,Y$ variabili aleatorie indipendenti, $Z=X+Y$

$f_Z(z) = \int_{-\infty}^{+\infty}f_X(z-y)f_Y(y)\,dy = \int_{-\infty}^{+\infty}f_Y(z-x)f_X(x)\,dx$

\underline{Casi particolari}

\begin{itemize}

\item $X\sim P(\lambda)\,,\quad Y\sim P(\mu)\Rightarrow Z\sim P(\lambda + \mu)$

\item $X\sim B(n,p)\,,\quad Y\sim B(m,p)\Rightarrow Z\sim B(n+m,p)$

\item $X\sim \text{Geo}(p)\,,\quad Y\sim \text{Geo}(p)\Rightarrow Z\sim \text{Pascal}(2,p)$

\item $X\sim \text{Pascal}(r,p)\,,\quad Y\sim \text{Pascal}(s,p)\Rightarrow Z\sim \text{Pascal}(r+s,p)$

\item $X\sim \mathcal{N}(\mu_X,\sigma_X^2)\,,\quad Y\sim \mathcal{N}(\mu_Y,\sigma_Y^2)\Rightarrow Z\sim \mathcal{N}(\mu,\sigma^2)$ con $\mu=\mu_X+\mu_Y$ e $\sigma^2=\sigma_X^2+\sigma_Y^2$

\item $X\sim \Gamma(\alpha,\lambda)\,,\quad Y\sim \Gamma(\beta,\lambda)\Rightarrow Z\sim \Gamma(\alpha+\beta,\lambda)$

\item $X\sim \text{Exp}(\lambda)\,,\quad Y\sim \text{Exp}(\lambda)\Rightarrow Z\sim \Gamma(2,\lambda)$

\item $X\sim\chi^2(m)\,,\quad Y\sim\chi^2(n)\Rightarrow Z\sim\chi^2(m+n)$

\item $X_1,\dots, X_n \sim_{iid} \mathcal{N}(0,1)\Rightarrow X_1^2,\dots, X_n^2 \sim_{iid}\Gamma(\frac{1}{2},\frac{1}{2})\,(\chi^2(1))\Rightarrow Y=X_1^2+\dots+X_n^2\sim\Gamma(\frac{n}{2},\frac{1}{2})$ 

$Y\sim\chi^2(n)$ Chi quadro con $n$ gradi di libertà

\end{itemize}

\subsection*{Distribuzioni}

\begin{multicols}{2}

\textbf{Bernoulli} $(p)$

\underline{Densità} $p_X(k) = \begin {cases}1-p & k =0\\ p & k=1\end {cases}$

\underline{Media} $p$

\underline{Varianza} $p(1-p)$

\underline{Funzione di ripartizione} $F_X(k)=(1-p)\, 0\leq k <1 $

\underline{Funzione caratteristica} $\Phi_X(t)= 1-p+pe^{it}$

\underline{Commenti} Fallimento e successo

\columnbreak

\textbf{Binomiale} $(n,p)$  

\underline{Densità} $p_X(k) = \binom {n}{k}p^k(1-p)^{n-k},\,k \in \{0,\dots,n\}$

\underline{Media} $np$

\underline{Varianza} $np(1-p)$

\underline{Funzione caratteristica} $\Phi_X(t)=((1-p)+pe^{it})^n$

\underline{Commenti} Numero di successi in $n$ lanci con probabilità $p$ di successo

\columnbreak

\textbf{Multinomiale} $(n,p_1,\dots,p_N)$

\underline{Densità} $f_{\underline {X}}(n_1,\dots,n_N)=$

$=\binom {n}{n_1 \dots n_{N+1}}\left(p_1^{n_1}\dots p_N^{n_N}\right)\left(1-\sum_{i=1}^N p_i\right)^{\left(n-\sum_{i=1}^N n_i\right)}=$

$=\binom {n}{n_1 \dots n_{N+1}}\left(p_1^{n_1}\dots p_N^{n_N}\right)p_{N+1}^{n_{N+1}}$

\underline{Commenti} $n$ prove ripetute e indipendenti con $N+1$ possibili risultati di probabilità, \quad $p_1,\dots,p_N \Rightarrow$

$p_{N+1}=1-\sum_{i=1}^Np_i$ ; la marginale della multinomiale è una binomiale
\\

\textbf{Pascal} $(p,n)$

\underline{Densità} $p_X(k) = \binom {n-1}{k-1}p^k(1-p)^{n-k}$

\underline{Media} $k\frac{1-p}{p^2}$

\underline{Varianza} $\sqrt{k}\frac{\sqrt{(1-p)}}{p}$

\underline{Commenti} Istante del $k$-esimo successo
\\

\textbf{Geometrica} $(p)$

\underline{Densità} $p_X(k) = p(1-p)^{k-1}, \, k =1,2,\dots$

\underline{Media} $\frac{1}{p}$

\underline{Varianza} $\frac{1-p}{p^2}$

\underline{Funzione di ripartizione} $F_X(k)=1-(1-p)^k$

\underline{Funzione caratteristica} $\Phi_X(t)= \frac{pe^{it}}{1-e^{it}(1-p)}$

\underline{Commenti} Numero di fallimenti che precedono il primo successo; la discretizzazione di una esponenziale è una geometrica
\\

\textbf{Ipergeometrica} $(n,h,r)$

\underline{Densità} $p_X(k) = \frac{\binom {h}{k}\binom {n-h}{r-k}}{\binom {n}{r}},\,k\leq h,\,0\leq r-k\leq n-h$

\underline{Media} $\frac{rh}{n}$

\underline{Varianza} $\frac{rh}{n}\left(1-\frac{h}{n}\right)\frac{n-r}{n-1}$

\underline{Commenti} Conta per $r$ elementi distinti estratti a caso senza reinserimento da un insieme con cardinalità $n$ quanti sono nel sottoinsieme di cardinalità $h$
\\

\textbf{Poisson} $(\lambda)$

\underline{Densità} $p_X(k) = e^{-\lambda}\frac{\lambda^k}{k!},\,k=0,1,\dots$

\underline{Media} $\lambda$

\underline{Varianza} $\lambda$

\underline{Funzione caratteristica} $\Phi_X(t)=e^{\lambda(e^{it}-1)}$

\underline{Commenti} Descrive il numero di eventi in un intervallo di tempo
\\

\textbf{Uniforme} $(a,b)$

\underline{Densità} $f_X(x) = \frac{1}{b-a}\boldsymbol{1}_{(a,b)}(x)$

\underline{Media} $\frac{a+b}{2}$

\underline{Varianza} $\frac{(b-a)^2}{12}$

\underline{Funzione di ripartizione} $F_X(x)=\frac{x-a}{b-a}$

\underline{Funzione caratteristica} $\Phi_X(t)=\frac{e^{ibt}-e^{iat}}{ibt-iat}$
\\

\textbf{Normale / Gaussiana} $(\mu,\sigma^2)$

\underline{Densità} $f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$

\underline{Media} $\mu$

\underline{Varianza} $\sigma^2$

\underline{Funzione caratteristica} $\Phi_X(t)=e^{\mu it-\frac{\sigma^2t^2}{2}}$
\\

\textbf{Chi quadro} $(k)$

\underline{Densità} $f_X(x) = \frac{1}{2^{\frac{k}{2}}\Gamma(\frac{k}{2})}x^{\frac{k}{2}-1}e^{-\frac{x}{2}}\boldsymbol{1}_{(0,+\infty)}(x)$, $k\in \mathbb{N}^+$

\underline{Media} $k$

\underline{Varianza} $2k$

\underline{Funzione caratteristica} $\Phi_X(t)= (1-2it)^{-\frac{k}{2}}$

\underline{Commenti} $\chi^2=\sum_{i=1}^k X_i^2\,,\,X_i\sim \mathcal{N}(0,1)\,iid$  e $k$ è detto \textit{numero di gradi di libertà}
\\

\textbf{Esponenziale} $(\lambda)$

\underline{Densità} $f_X(x) = \lambda e^{-\lambda x}\boldsymbol{1}_{(0,+\infty)}(x)$

\underline{Media} $\frac{1}{\lambda}$

\underline{Varianza} $\frac{1}{\lambda^2}$

\underline{Funzione di ripartizione} $F_X(x)=1-e^{-\lambda x}$

\underline{Funzione caratteristica} $\Phi_X(t)=\frac{\lambda}{\lambda-it}$

\underline{Commenti} Descrive il tempo di attesa tra due eventi successivi

La discretizzazione di una esponenziale è una geometrica
\\

\textbf{Gamma} $(\alpha, \lambda)$

\underline{Densità} $f_X(x) = \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}\boldsymbol{1}_{(0,+\infty)}(x)$,

con $\Gamma(\alpha)=\int_0^{+\infty} y^{\alpha-1}e^{-y}\,dy$

\underline{Media} $\frac{\alpha}{\lambda}$

\underline{Varianza} $\frac{\alpha}{\lambda^2}$

\underline{Funzione caratteristica} $\Phi_X(t)=\left(1-\frac{it}{\lambda}\right)^{-k}$

\underline{Commenti} Se $\alpha$ è intero si parla di Erlang

\end{multicols}

\textbf{Proprietà funzione $\Gamma(\alpha)$ di Eulero} 

$\Gamma(1)=1$ , \quad 
$\Gamma(\alpha+1) = \alpha\Gamma(\alpha)$ , \quad
$\Gamma(n)=(n-1)!\,,\,n\in \mathbb{N}$ , \quad 
$\Gamma(\frac{1}{2})=\sqrt {\pi}$ , \quad 
$\Gamma(\frac{3}{2})=\frac{1}{2}\Gamma(\frac{1}{2})=\frac{1}{2}\pi,\dots$

\textbf{Proprietà distribuzione} $\Gamma(\alpha,\lambda)$

$\Gamma(1,\lambda) \sim \text{Exp}(\lambda)$ , \quad $X\sim \mathcal {N}(0,1)\Rightarrow X^2\sim\Gamma(\frac{1}{2},\frac{1}{2})\sim\chi^2(1)$ 

$\Gamma(n,\lambda),\,n\in \mathbb{N},$ è una \textit{distribuzione di Erlang}, $\alpha$ si dice \textit{parametro di forma} e $\lambda$ \textit{parametro di scala} (tempo di attesa per la realizzazione di $n$ eventi in un processo di Poisson)

$X_1,\dots,X_n$ variabili aleatorie indipendenti con distribuzione $\Gamma(\alpha_i,\lambda)\Rightarrow \sum_{i=1}^nX_i\sim\Gamma(\alpha_1+\dots+\alpha_n,\lambda)$

\pagebreak

\textbf{Trasformazione di variabili e vettori aleatori}

\begin{itemize}

\item \underline{Variabile aleatoria discreta} $f_Y(y)=\mathbb{P}_Y(\{y\})=\mathbb{P}_X(g^{-1}(\{y\}))=\sum_{x\in g^{-1}(\{y\})}f_X(x)$
 
\item \underline{Variabile aleatoria assolutamente continua} 

\begin{itemize}

\item \textit{Metodo della funzione di ripartizione} e poi si deriva per ottenere $f_Y(y)$

\item se $g:\mathbb{R}\rightarrow \mathbb{R}$ t.c. $g\in C^1(\mathbb{R})$, $g$ strettamente monotona e con inversa di classe $C^1$

$f_Y(y)=f_X(g^{-1}(y))|\frac{d}{dy}g^{-1}(y)|\,\boldsymbol{1}_{g(\mathbb{R)}}(y)$

N.B. $\frac{d}{dy}g^{-1}(y) = \frac{1}{g'(g^{-1}(y))}$

la relazione è valida anche per partizioni in cui $g$ soddisfa le ipotesi

\end{itemize}

\item \underline{Vettore aleatorio assolutamente continuo} $W = (S,T) = g(X,Y)$, $g$ è un $C^1$-diffeomorfismo, cioè è differenziabile con continuità, invertibile con determinante Jacobiano non nullo su $A$ e inversa $C^1$

$f_{(S,T)}(s,t)=f_{(X,Y)}(g^{-1}(s,t))\left|\text{det} J_{g^{-1}}(s,t)\right|\boldsymbol{1}_{g(A)}(s,t)$

N.B. $\text{det}J_{g^{-1}}(s,t)=\frac{1}{\text{det}J_g(g^{-1}(s,t))}$

\end{itemize}

\textbf{Trasformata di Fourier} $\mu$ misura finita su $(\mathbb{R}^N, \mathcal{B}(\mathbb{R}^N))$\quad $\hat {\mu}:\mathbb{R}^N\rightarrow \mathbb{C}$ t.c. $\hat {\mu}(\underline{t})=\int_{\mathbb{R}^N}e^{i \underline{t}^T \underline{x}}\,d\mu(\underline{x})$

\textbf{Funzione caratteristica} $\underline{X}$ v. a. vettoriale a valori in $\mathbb{R}^N$, trasformata di Fourier della sua legge $\mathbb{P}_{\underline{X}}$

$\Phi_{\underline{X}}(t)=\mathbb{E}\left[e^{i \underline{t}^T \underline{X}}\right]$

\textbf{Proprietà funzione caratteristica}

\begin{itemize}

\item $\Phi_{\underline{X}}$ limitata, (uniformemente) continua, $\Phi_{\underline{X}}(\underline{0})=1$

\item $\Phi_{-\underline{X}}(\underline{t}) = \overline{\Phi_{\underline{X}}(\underline{t})}$\quad$ \Rightarrow$\quad se $\underline{X}$ è simmetrica, cioè $\underline{X}\sim-\underline{X}$ , \quad $\Phi_{\underline{X}}(\underline{t})\in \mathbb{R}$

\item $\underline{X}\ind \underline{Y}$ \quad $\Rightarrow$ \quad $\Phi_{\underline{X}+\underline{Y}}(\underline{t})=\Phi_{\underline{X}}(\underline{t})\cdot\Phi_{\underline{X}}(\underline{t})$

\item $\Phi_{A\underline{X}+\underline{b}}(\underline{t})= e^{i \underline{t}^T \underline{b}}\,\Phi_{\underline{X}}(A^T\underline{t})$

\item $\Phi_{X_j}(t) = \Phi_{\underline{X}}(t\underline{e}_j)$

\item $\underline{X}$ vettore aleatorio a valori in $\mathbb{R}^N$, se le componenti ammettono momento di ordine $m$ la f.c. ammette tutte le derivate parziali continue fino all'ordine $m$

$\frac{\partial^k}{\partial t_{j_1}\dots\partial t_{j_k}}\Phi_{\underline{X}}(\underline{t}) =i^k \mathbb{E}\left[X_{j_1}\dots X_{j_k}e^{i \underline{t}^T \underline{X}}\right]$ \quad $\forall 1\leq k\leq m$

\begin{itemize}

\item $X$ v.a. reale, $X\in L^m(\Omega,\mathcal{F},\mathbb{P})$ \quad $\Rightarrow$ \quad $ \forall 1\leq k \leq m$ \quad $\Phi_X^{(k)}(t)=\frac{d^k}{dt^k}\Phi_X(t)=i^k \mathbb{E}\left[X^ke^{itX}\right]$ 

e in particolare $\Phi_X^{(k)}(0)=i^k \mathbb{E}\left[X^k\right]$ 

\item se $X$ v.a. reale e $\Phi_X$ ammette derivata continua fino all'ordine $m=2k$ $\Rightarrow$ $X$ ammette momento di ordine $m=2k$ (N.B. solo ordine pari, es. derivabile 5 volte $\Rightarrow$ massimo momento 4)

\end{itemize}

\item $f_{\underline{X}}(\underline{x})=\frac{1}{(2\pi)^N}\int_{\mathbb{R}^N}e^{-i \underline{t}^T \underline{x}}\Phi_{\underline{X}}(\underline{t})\,d \underline{t}$

\item $\Phi_{\underline{X}}(\underline{t})=\mathbb{E}\left[e^{i \underline{t}^T \underline{X}}\right]=\int_{\mathbb{R}^N}e^{i \underline{t}^T \underline{x}}f_{\underline{X}}(\underline{x})\,d \underline{x}$

\item $X_j$ indipendenti \quad $\Leftrightarrow$ \quad $\Phi_{\underline{X}}(\underline{t})=\prod_{j=1}^N \Phi_{X_j}(t_j)$ \quad $\forall \underline{t}\in \mathbb{R}^N$

\item $\Phi_{T}$ è integrabile $\Rightarrow$ $T$ è assolutamente continua e $f_T(x)=\frac{1}{2\pi}\int_{-\infty}^{+\infty}e^{-itx}\Phi_T(t)\,dt$

\end{itemize}

\pagebreak

\textbf{Condizionamento}

\begin{itemize}

\item \underline{Caso discreto}

\begin{itemize}

\item $f_{X|Y=y} = \frac{f_(X,Y)(x,y)}{f_Y(y)}$ se $f_Y(y)\neq 0$ è detta \textit{densità discreta di $X$ condizionata all'evento ($Y=y$)} e definisce una \textit{legge} $\mathbb{P}_{X|Y=y}(A) = \displaystyle\sum_{x\in A}f_{X|Y=y}(x) \quad \forall A\in \mathcal{B}(\mathbb{R})$

\item $\mathbb{E}[X|Y=y] = \displaystyle\sum_{x\in X(\Omega)}xf_{X|Y=y}(x) = h(y)$ \textit{attesa di $X$ condizionata a $(Y=y)$} , $h: \mathbb{R}\rightarrow \mathbb{R}$ misurabile se la serie è assolutamente convergente

\item $h(Y)=h\,\circ\,Y = \mathbb{E}[X|Y]$ \textit{attesa di X condizionata a Y} (N.B. variabile aleatoria!) $\sigma(Y)$-misurabile

\item $\int_A \mathbb{E}[X|Y]\,d \mathbb{P} = \int_AX\,d \mathbb{P}\Rightarrow \mathbb{E}[\mathbb{E}[X|Y]\boldsymbol{1}_A] = \mathbb{E}[X \boldsymbol{1}_A]$\quad$\forall A \in \sigma(Y)$, $A=Y^{-1}(B)$ con $B\in \mathcal{B}(\mathbb{R})$

Questa caratterizzazione si può estendere ad una qualsiasi variabile aleatoria $Y$ e più in generale si può estendere sostituendo $\sigma(Y)$ con una $\sigma$-algebra generica

\end{itemize}

\item \underline{Attesa condizionata rispetto a una $\sigma$-algebra} 

\begin{itemize}

\item $f_{(X,Y)}(x,y) = f_{X|Y=y}(x)f_Y(y)$ q.o.
	
\item $X\ind Y \Leftrightarrow f_{X|Y=y}(x)=f_X(x)$ q.o.

\item Calcolo di $\mathbb{E}[X|Y]$: $f_{X|Y=y}\rightarrow h(y)=\mathbb{E}[X|Y=y] = \int_\mathbb{R}xf_{X|Y=y}(x)\,dx \rightarrow h\circ Y = h(Y)=\mathbb{E}[X|Y]$

\end{itemize}

\end{itemize}



\huge aggiornato alla lezione 17

\end{document}

