\documentclass[openany]{book} % oppure \openany in mezzo al libro
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage[T1]{fontenc}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{geometry}
\usepackage{amsfonts}
\usepackage{multicol}
\usepackage[none]{hyphenat}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{old-arrows}
\usepackage{wrapfig}

\DeclareMathSymbol{\nrightarrow} {\mathrel}{AMSb}{"39}


\sloppy
%\raggedright 
 \geometry{
 a4paper,
 left=20mm,
 right=20mm,
 top=20mm,
 bottom=20mm,
}

\newcommand{\ind}{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}

\pagestyle{empty}

\title{Titolo}
\date{}
\author{MT}
\renewcommand{\baselinestretch}{1.2}

\setlength{\parindent}{0em}
%\setlength{\parskip}{0.2em}

\setlist{nosep}

\begin{document}

\begin{center}
\Large Probabilità e Statistica 

\large Formulario
\end{center}

$e^{ix} = \cos(x)+i\sin(x)$

\textbf{Disposizioni con ripetizione} $D^R_{n.k}=n^k$

\textbf{Disposizioni semplici} $D_{n,k}=\frac{n!}{(n-k)!}$

\textbf{Permutazioni di n elementi} \quad $P_n = D_{n,n}=n!$

\textbf{Combinazioni semplici} (numero di sottoinsiemi di cardinalità $k$) \quad $C_{n,k}=\binom {n}{k} = \frac{n!}{k!(n-k)!}$

\textbf{Coefficiente multinomiale} (numero di modi in cui si può partizionare un insieme in $r$ sottoinsiemi di cardinalità $k_1,\dots,k_n$) \quad $\binom {n}{k_1,\dots,k_n}=\frac{n!}{k_1 ! \dots k_n !}$ 

\textbf{Formula di Poincaré} (principio di inclusione - esclusione) \quad $\mu(A_1 \cup \dots \cup A_n) =$ 

$= \sum_{k=1}^n (-1)^{k+1}\sum_{1\leq i_1 < \dots <i_k \leq n}\mu(A_{i_1}\cap \dots \cap A_{i_k})$, $\quad \mu$ misura finita

\textbf{Valore atteso} $\mathbb{E}(X) = \int_{-\infty}^{+\infty}xf(x)\,dx$

\textbf{Varianza} $\text{Var}(X)=\mathbb{E}(X^2)-\mathbb{E}(X)^2$

\textbf{Covarianza} $\text{Cov}(X,Y)=\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)$

\textbf{Coefficiente di correlazione} $\rho_{XY} = \frac{\text{Cov}(X,Y)}{\sqrt {\text{Var}(X)\text{Var}(X)}}$

\textbf{Algebra dei valori attesi}

$\mathbb {E}(aX+bY+c)= a\mathbb{E}(X)+b \mathbb{E}(Y)+c$

$\text {Var}(aX+bY+c)= a^2 \text{Var}(X)+b^2 \text{Var}(Y)+2ab\text{Cov}(X,Y)$

$\text{Cov}(aX+bY+c,Z) = a \text{Cov}(X,Z)+b \text{Cov(Y,Z)}$

\textbf{Convoluzione} $X,\,Y$ variabili aleatorie indipendenti, $Z=X+Y$

$f_Z(z) = \int_{-\infty}^{+\infty}f_X(z-y)f_Y(y)\,dy = \int_{-\infty}^{+\infty}f_Y(z-x)f_X(x)\,dx$

\underline{Casi particolari}

\begin{itemize}

\item $X\sim P(\lambda)\,,\quad Y\sim P(\mu)\Rightarrow Z\sim P(\lambda + \mu)$

\item $X\sim B(n,p)\,,\quad Y\sim B(m,p)\Rightarrow Z\sim B(n+m,p)$

\item $X\sim \text{Geo}(p)\,,\quad Y\sim \text{Geo}(p)\Rightarrow Z\sim \text{Pascal}(2,p)$

\item $X\sim \text{Pascal}(r,p)\,,\quad Y\sim \text{Pascal}(s,p)\Rightarrow Z\sim \text{Pascal}(r+s,p)$

\item $X\sim \mathcal{N}(\mu_X,\sigma_X^2)\,,\quad Y\sim \mathcal{N}(\mu_Y,\sigma_Y^2)\Rightarrow Z\sim \mathcal{N}(\mu,\sigma^2)$ con $\mu=\mu_X+\mu_Y$ e $\sigma^2=\sigma_X^2+\sigma_Y^2$

\item $X\sim \Gamma(\alpha,\lambda)\,,\quad Y\sim \Gamma(\beta,\lambda)\Rightarrow Z\sim \Gamma(\alpha+\beta,\lambda)$

\item $X\sim \text{Exp}(\lambda)\,,\quad Y\sim \text{Exp}(\lambda)\Rightarrow Z\sim \Gamma(2,\lambda)$

\item $X\sim\chi^2(m)\,,\quad Y\sim\chi^2(n)\Rightarrow Z\sim\chi^2(m+n)$

\item $X_1,\dots, X_n \sim_{iid} \mathcal{N}(0,1)\Rightarrow X_1^2,\dots, X_n^2 \sim_{iid}\Gamma(\frac{1}{2},\frac{1}{2})\,(\chi^2(1))\Rightarrow Y=X_1^2+\dots+X_n^2\sim\Gamma(\frac{n}{2},\frac{1}{2})$ 

$Y\sim\chi^2(n)$ Chi quadro con $n$ gradi di libertà

\end{itemize}

\begin{multicols}{2}

\textbf{Bernoulli} $(p)$

\underline{Densità} $p_X(k) = \begin {cases}1-p & k =0\\ p & k=1\end {cases}$

\underline{Media} $p$

\underline{Varianza} $p(1-p)$

\underline{Funzione di ripartizione} $F_X(k)=(1-p)\, 0\leq k <1 $

\underline{Funzione caratteristica} $\Phi_X(t)= 1-p+pe^{it}$

\underline{Commenti} Fallimento e successo
\\

\textbf{Binomiale} $(n,p)$  

\underline{Densità} $p_X(k) = \binom {n}{k}p^k(1-p)^{n-k},\,k \in \{0,\dots,n\}$

\underline{Media} $np$

\underline{Varianza} $np(1-p)$

\underline{Funzione caratteristica} $\Phi_X(t)=((1-p)+pe^{it})^n$

\underline{Commenti} Numero di successi in $n$ lanci con probabilità $p$ di successo
\\

\textbf{Multinomiale} $(n,p_1,\dots,p_N)$

\underline{Densità} $f_{\underline {X}}(n_1,\dots,n_N)=$

$=\binom {n}{n_1 \dots n_{N+1}}\left(p_1^{n_1}\dots p_N^{n_N}\right)\left(1-\sum_{i=1}^N p_i\right)^{\left(n-\sum_{i=1}^N n_i\right)}=$

$=\binom {n}{n_1 \dots n_{N+1}}\left(p_1^{n_1}\dots p_N^{n_N}\right)p_{N+1}^{n_{N+1}}$

\underline{Commenti} $n$ prove ripetute e indipendenti con $N+1$ possibili risultati di probabilità, \quad $p_1,\dots,p_N \Rightarrow$

$p_{N+1}=1-\sum_{i=1}^Np_i$ ; la marginale della multinomiale è una binomiale
\\

\textbf{Pascal} $(p,n)$

\underline{Densità} $p_X(k) = \binom {n-1}{k-1}p^k(1-p)^{n-k}$

\underline{Media} $k\frac{1-p}{p^2}$

\underline{Varianza} $\sqrt{k}\frac{\sqrt{(1-p)}}{p}$

\underline{Commenti} Istante del $k$-esimo successo
\\

\textbf{Geometrica} $(p)$

\underline{Densità} $p_X(k) = p(1-p)^{k-1}, \, k =1,2,\dots$

\underline{Media} $\frac{1}{p}$

\underline{Varianza} $\frac{1-p}{p^2}$

\underline{Funzione di ripartizione} $F_X(k)=1-(1-p)^k$

\underline{Funzione caratteristica} $\Phi_X(t)= \frac{pe^{it}}{1-e^{it}(1-p)}$

\underline{Commenti} Numero di fallimenti che precedono il primo successo; la discretizzazione di una esponenziale è una geometrica
\\

\textbf{Ipergeometrica} $(n,h,r)$

\underline{Densità} $p_X(k) = \frac{\binom {h}{k}\binom {n-h}{r-k}}{\binom {n}{r}},\,k\leq h,\,0\leq r-k\leq n-h$

\underline{Media} $\frac{rh}{n}$

\underline{Varianza} $\frac{rh}{n}\left(1-\frac{h}{n}\right)\frac{n-r}{n-1}$

\underline{Commenti} Conta per $r$ elementi distinti estratti a caso senza reinserimento da un insieme con cardinalità $n$ quanti sono nel sottoinsieme di cardinalità $h$
\\

\textbf{Poisson} $(\lambda)$

\underline{Densità} $p_X(k) = e^{-\lambda}\frac{\lambda^k}{k!},\,k=0,1,\dots$

\underline{Media} $\lambda$

\underline{Varianza} $\lambda$

\underline{Funzione caratteristica} $\Phi_X(t)=e^{\lambda(e^{it}-1)}$

\underline{Commenti} Descrive il numero di eventi in un intervallo di tempo
\\

\textbf{Uniforme} $(a,b)$

\underline{Densità} $f_X(x) = \frac{1}{b-a}\boldsymbol{1}_{(a,b)}(x)$

\underline{Media} $\frac{a+b}{2}$

\underline{Varianza} $\frac{(b-a)^2}{12}$

\underline{Funzione di ripartizione} $F_X(x)=\frac{x-a}{b-a}$

\underline{Funzione caratteristica} $\Phi_X(t)=\frac{e^{ibt}-e^{iat}}{ibt-iat}$
\\

\textbf{Normale / Gaussiana} $(\mu,\sigma^2)$

\underline{Densità} $f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$

\underline{Media} $\mu$

\underline{Varianza} $\sigma^2$

\underline{Funzione caratteristica} $\Phi_X(t)=e^{\mu it-\frac{\sigma^2t^2}{2}}$
\\

\textbf{Chi quadro} $(k)$

\underline{Densità} $f_X(x) = \frac{1}{2^{\frac{k}{2}}\Gamma(\frac{k}{2})}x^{\frac{k}{2}-1}e^{-\frac{x}{2}}\boldsymbol{1}_{(0,+\infty)}(x)$, $k\in \mathbb{N}^+$

\underline{Media} $k$

\underline{Varianza} $2k$

\underline{Funzione caratteristica} $\Phi_X(t)= (1-2it)^{-\frac{k}{2}}$

\underline{Commenti} $\chi^2=\sum_{i=1}^k X_i^2\,,\,X_i\sim \mathcal{N}(0,1)\,iid$  e $k$ è detto \textit{numero di gradi di libertà}
\\

\textbf{Esponenziale} $(\lambda)$

\underline{Densità} $f_X(x) = \lambda e^{-\lambda x}\boldsymbol{1}_{(0,+\infty)}(x)$

\underline{Media} $\frac{1}{\lambda}$

\underline{Varianza} $\frac{1}{\lambda^2}$

\underline{Funzione di ripartizione} $F_X(x)=1-e^{-\lambda x}$

\underline{Funzione caratteristica} $\Phi_X(t)=\frac{\lambda}{\lambda-it}$

\underline{Commenti} Descrive il tempo di attesa tra due eventi successivi

La discretizzazione di una esponenziale è una geometrica
\\

\textbf{Gamma} $(\alpha, \lambda)$

\underline{Densità} $f_X(x) = \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}\boldsymbol{1}_{(0,+\infty)}(x)$,

con $\Gamma(\alpha)=\int_0^{+\infty} y^{\alpha-1}e^{-y}\,dy$

\underline{Media} $\frac{\alpha}{\lambda}$

\underline{Varianza} $\frac{\alpha}{\lambda^2}$

\underline{Funzione caratteristica} $\Phi_X(t)=\left(1-\frac{it}{\lambda}\right)^{-k}$

\underline{Commenti} Se $\alpha$ è intero si parla di Erlang

\end{multicols}

\textbf{Proprietà funzione $\Gamma(\alpha)$ di Eulero} 

$\Gamma(1)=1$ , \quad 
$\Gamma(\alpha+1) = \alpha\Gamma(\alpha)$ , \quad
$\Gamma(n)=(n-1)!\,,\,n\in \mathbb{N}$ , \quad 
$\Gamma(\frac{1}{2})=\sqrt {\pi}$ , \quad 
$\Gamma(\frac{3}{2})=\frac{1}{2}\Gamma(\frac{1}{2})=\frac{1}{2}\pi,\dots$

\textbf{Proprietà distribuzione} $\Gamma(\alpha,\lambda)$

$\Gamma(1,\lambda) \sim \text{Exp}(\lambda)$ , \quad $X\sim \mathcal {N}(0,1)\Rightarrow X^2\sim\Gamma(\frac{1}{2},\frac{1}{2})\sim\chi^2(1)$ 

$\Gamma(n,\lambda),\,n\in \mathbb{N},$ è una \textit{distribuzione di Erlang}, $\alpha$ si dice \textit{parametro di forma} e $\lambda$ \textit{parametro di scala} (tempo di attesa per la realizzazione di $n$ eventi in un processo di Poisson)

$X_1,\dots,X_n$ variabili aleatorie indipendenti con distribuzione $\Gamma(\alpha_i,\lambda)\Rightarrow \sum_{i=1}^nX_i\sim\Gamma(\alpha_1+\dots+\alpha_n,\lambda)$

\textbf{Trasformazione di variabili e vettori aleatori}

\begin{itemize}

\item \underline{Variabile aleatoria discreta} $f_Y(y)=\mathbb{P}_Y(\{y\})=\mathbb{P}_X(g^{-1}(\{y\}))=\sum_{x\in g^{-1}(\{y\})}f_X(x)$
 
\item \underline{Variabile aleatoria assolutamente continua} 

\begin{itemize}

\item \textit{Metodo della funzione di ripartizione} e poi si deriva per ottenere $f_Y(y)$

\item se $g:\mathbb{R}\rightarrow \mathbb{R}$ t.c. $g\in C^1(\mathbb{R})$, $g$ strettamente monotona e con inversa di classe $C^1$

$f_Y(y)=f_X(g^{-1}(y))|\frac{d}{dy}g^{-1}(y)|\,\boldsymbol{1}_{g(\mathbb{R)}}(y)$

N.B. $\frac{d}{dy}g^{-1}(y) = \frac{1}{g'(g^{-1}(y))}$

la relazione è valida anche per partizioni in cui $g$ soddisfa le ipotesi

\end{itemize}

\item \underline{Vettore aleatorio assolutamente continuo} $W = (S,T) = g(X,Y)$, $g$ è un $C^1$-diffeomorfismo, cioè è differenziabile con continuità, invertibile con determinante Jacobiano non nullo su $A$ e inversa $C^1$

$f_{(S,T)}(s,t)=f_{(X,Y)}(g^{-1}(s,t))\left|\text{det} J_{g^{-1}}(s,t)\right|\boldsymbol{1}_{g(A)}(s,t)$

N.B. $\text{det}J_{g^{-1}}(s,t)=\frac{1}{\text{det}J_g(g^{-1}(s,t))}$

\end{itemize}

\textbf{Trasformata di Fourier} $\mu$ misura finita su $(\mathbb{R}^N, \mathcal{B}(\mathbb{R}^N))$\quad $\hat {\mu}:\mathbb{R}^N\rightarrow \mathbb{C}$ t.c. $\hat {\mu}(\underline{t})=\int_{\mathbb{R}^N}e^{i \underline{t}^T \underline{x}}\,d\mu(\underline{x})$

\textbf{Funzione caratteristica} $\underline{X}$ v. a. vettoriale a valori in $\mathbb{R}^N$, trasformata di Fourier della sua legge $\mathbb{P}_{\underline{X}}$

$\Phi_{\underline{X}}(t)=\mathbb{E}\left[e^{i \underline{t}^T \underline{X}}\right]$

\textbf{Proprietà funzione caratteristica}

\begin{itemize}

\item $\Phi_{\underline{X}}$ limitata, (uniformemente) continua, $\Phi_{\underline{X}}(\underline{0})=1$

\item $\Phi_{-\underline{X}}(\underline{t}) = \overline{\Phi_{\underline{X}}(\underline{t})}$\quad$ \Rightarrow$\quad se $\underline{X}$ è simmetrica, cioè $\underline{X}\sim-\underline{X}$ , \quad $\Phi_{\underline{X}}(\underline{t})\in \mathbb{R}$

\item $\underline{X}\ind \underline{Y}$ \quad $\Rightarrow$ \quad $\Phi_{\underline{X}+\underline{Y}}(\underline{t})=\Phi_{\underline{X}}(\underline{t})\cdot\Phi_{\underline{X}}(\underline{t})$

\item $\Phi_{A\underline{X}+\underline{b}}(\underline{t})= e^{i \underline{t}^T \underline{b}}\,\Phi_{\underline{X}}(A^T\underline{t})$

\item $\Phi_{X_j}(t) = \Phi_{\underline{X}}(t\underline{e}_j)$

\item $\underline{X}$ vettore aleatorio a valori in $\mathbb{R}^N$, se le componenti ammettono momento di ordine $m$ la f.c. ammette tutte le derivate parziali continue fino all'ordine $m$

$\frac{\partial^k}{\partial t_{j_1}\dots\partial t_{j_k}}\Phi_{\underline{X}}(\underline{t}) =i^k \mathbb{E}\left[X_{j_1}\dots X_{j_k}e^{i \underline{t}^T \underline{X}}\right]$ \quad $\forall 1\leq k\leq m$

\begin{itemize}

\item $X$ v.a. reale, $X\in L^m(\Omega,\mathcal{F},\mathbb{P})$ \quad $\Rightarrow$ \quad $ \forall 1\leq k \leq m$ \quad $\Phi_X^{(k)}(t)=\frac{d^k}{dt^k}\Phi_X(t)=i^k \mathbb{E}\left[X^ke^{itX}\right]$ 

e in particolare $\Phi_X^{(k)}(0)=i^k \mathbb{E}\left[X^k\right]$ 

\item se $X$ v.a. reale e $\Phi_X$ ammette derivata continua fino all'ordine $m=2k$ $\Rightarrow$ $X$ ammette momento di ordine $m=2k$ (N.B. solo ordine pari, es. derivabile 5 volte $\Rightarrow$ massimo momento 4)

\end{itemize}

\item $f_{\underline{X}}(\underline{x})=\frac{1}{(2\pi)^N}\int_{\mathbb{R}^N}e^{-i \underline{t}^T \underline{x}}\Phi_{\underline{X}}(\underline{t})\,d \underline{t}$

\item $\Phi_{\underline{X}}(\underline{t})=\mathbb{E}\left[e^{i \underline{t}^T \underline{X}}\right]=\int_{\mathbb{R}^N}e^{i \underline{t}^T \underline{x}}f_{\underline{X}}(\underline{x})\,d \underline{x}$

\item $X_j$ indipendenti \quad $\Leftrightarrow$ \quad $\Phi_{\underline{X}}(\underline{t})=\prod_{j=1}^N \Phi_{X_j}(t_j)$ \quad $\forall \underline{t}\in \mathbb{R}^N$

\item $\Phi_{T}$ è integrabile $\Rightarrow$ $T$ è assolutamente continua e $f_T(x)=\frac{1}{2\pi}\int_{-\infty}^{+\infty}e^{-itx}\Phi_T(t)\,dt$

\end{itemize}

\textbf{Condizionamento}

\begin{itemize}

\item \underline{Caso discreto}

\begin{itemize}

\item $f_{X|Y=y} = \frac{f_(X,Y)(x,y)}{f_Y(y)}$ se $f_Y(y)\neq 0$ è detta \textit{densità discreta di $X$ condizionata all'evento ($Y=y$)} e definisce una \textit{legge} $\mathbb{P}_{X|Y=y}(A) = \displaystyle\sum_{x\in A}f_{X|Y=y}(x) \quad \forall A\in \mathcal{B}(\mathbb{R})$

\item $\mathbb{E}[X|Y=y] = \displaystyle\sum_{x\in X(\Omega)}xf_{X|Y=y}(x) = h(y)$ \textit{attesa di $X$ condizionata a $(Y=y)$} , $h: \mathbb{R}\rightarrow \mathbb{R}$ misurabile se la serie è assolutamente convergente

\item $h(Y)=h\,\circ\,Y = \mathbb{E}[X|Y]$ \textit{attesa di X condizionata a Y} (N.B. variabile aleatoria!) $\sigma(Y)$-misurabile

\item $\int_A \mathbb{E}[X|Y]\,d \mathbb{P} = \int_AX\,d \mathbb{P}\Rightarrow \mathbb{E}[\mathbb{E}[X|Y]\boldsymbol{1}_A] = \mathbb{E}[X \boldsymbol{1}_A]$\quad$\forall A \in \sigma(Y)$, $A=Y^{-1}(B)$ con $B\in \mathcal{B}(\mathbb{R})$

Questa caratterizzazione si può estendere ad una qualsiasi variabile aleatoria $Y$ e più in generale si può estendere sostituendo $\sigma(Y)$ con una $\sigma$-algebra generica

\end{itemize}

\item \underline{Attesa condizionata rispetto a una $\sigma$-algebra} 

\begin{itemize}

\item $f_{(X,Y)}(x,y) = f_{X|Y=y}(x)f_Y(y)$ q.o.
	
\item $X\ind Y \Leftrightarrow f_{X|Y=y}(x)=f_X(x)$ q.o.

\item Calcolo di $\mathbb{E}[X|Y]$: $f_{X|Y=y}\rightarrow h(y)=\mathbb{E}[X|Y=y] = \int_\mathbb{R}xf_{X|Y=y}(x)\,dx \rightarrow h\circ Y = h(Y)=\mathbb{E}[X|Y]$

\item $\mathbb{E}[Y]=\mathbb{E}[\mathbb{E}[Y|X]]$

\item $\text{Var}(Y)=\mathbb{E}[\text{Var}(Y|X)] + \text{Var}(\mathbb{E}[Y|X])$

\item $\text{Var}(Y|X)=\mathbb{E}[Y^2|X]-(\mathbb{E}[Y|X])^2$

\end{itemize}

\end{itemize}

\textbf{Convergenza quasi ovunque (quasi certamente)} $\mathbb{P}(\{\omega\in\Omega:X_n(\omega) \nrightarrow X(\omega)\}) = \mathbb{P}(X_N\nrightarrow X)=0$

$X_n \overset{q.o.}\rightarrow X\quad X_n \overset{q.c.}\rightarrow X$

\textbf{Convergenza in probabilità (in misura)} $\mathbb{P}(\{\omega\in\Omega:|X_n(\omega)-X(\omega)|>\epsilon)=\mathbb{P}(|X_n-X|>\epsilon)\underset{n \rightarrow \infty} \rightarrow 0$

$X_n\overset {\mathbb{P}}\rightarrow X$

\textbf{Disuguaglianza di Markov} $X$ v.a., $X\in L^1(\Omega,\mathcal{F},\mathbb{P})$ e $X\geq 0$ q.o., allora $\forall\epsilon>0 \quad \mathbb{P}(X>\epsilon)\leq \frac{\mathbb{E}[X]}{\epsilon}$

\textbf{Disuguaglianza di Cebicev} $X$ v.a., $X\in L^2(\Omega,\mathcal{F},\mathbb{P})$, allora $\forall \epsilon>0 \quad \mathbb{P}(|X-\mathbb{E}[X]|>\epsilon)\leq \frac{\text{Var}(X)}{\epsilon^2}$

\pagebreak

\textbf{Convergenza in norma $L^p$} $X_N,\,X\in L^p\quad ||X_n-X||_{L^p}\underset {n \rightarrow\infty}\rightarrow 0$, ovvero $\mathbb{E}[|X_n-X|^p]\underset {n \rightarrow\infty}\rightarrow 0$ 

$X_n\overset {L^p} \rightarrow X$ \quad (N.B. $||X||_{L^p}=(\mathbb{E}[|X|^p])^{\frac{1}{p}}$)

\begin{enumerate}

\item $p \geq q \geq 1$ \quad $X_n\overset {L^p} \rightarrow X \Rightarrow X_n\overset {L^q} \rightarrow X$

\item $X_n\overset {L^1} \rightarrow X \Rightarrow \mathbb{E}[X_n]\underset {n \rightarrow\infty} \rightarrow \mathbb{E}[X]$

\item $X_n\overset {L^p} \rightarrow X,$ $p\geq 1 \Rightarrow \mathbb{E}[|X_n|^p]\underset {n \rightarrow\infty} \rightarrow \mathbb{E}[|X|^p]$, ovvero $||X_n||_{L^p}\rightarrow||X||_{L^p}$

in particolare se converge in $L^2$, converge anche in $L^1$, quindi $\mathbb{E}[X_n]\underset {n \rightarrow\infty} \rightarrow \mathbb{E}[X]$, $\mathbb{E}[X_n^2]\underset {n \rightarrow\infty} \rightarrow \mathbb{E}[X^2]$ $\Rightarrow$ $\text{Var}(X_n)\rightarrow \text{Var}(X)$

\item $X_n\overset {L^2} \rightarrow a,$ $a\in \mathbb{R} \Leftrightarrow \mathbb{E}[X_n]\rightarrow a, \text{Var}(X_n)\rightarrow 0$

\end{enumerate}

\textbf{Caratterizzazione della convergenza in probabilità} \quad $X_n\overset {\mathbb{P}}\rightarrow X \Leftrightarrow \mathbb{E}\left[\frac{|X_n-X|}{|X_n-X|+1}\right]\underset {n \rightarrow\infty}\rightarrow 0$

\begin{itemize}

\item $X_n\overset {L^p} \rightarrow X,$ $p\geq 1 \Rightarrow X_n\overset {\mathbb{P}}\rightarrow X$

\item $X_n\overset {q.o.} \rightarrow X \Rightarrow X_n\overset {\mathbb{P}}\rightarrow X$

\item $X_n\overset {\mathbb{P}}\rightarrow X$ e $|X_n|\leq Y$ $\forall n\in \mathbb{N}$ con $Y\in L^p \Rightarrow X_n\overset {L^p}\rightarrow X$

\item $X_n\overset {\mathbb{P}}\rightarrow X$ e $|X_n|\leq M$ $\forall n\in \mathbb{N}$ con $M\in \mathbb{R}$ (uniformemente limitata) $ \Rightarrow X_n\overset {L^p}\rightarrow X\, \forall p\geq 1$

\item $X_n\overset {\mathbb{P}}\rightarrow X \Rightarrow \exists$ una sottosuccessione $(X_{n_k})_k$ t.c. $X_{n_k}\overset {q.c.}\rightarrow X$

\end{itemize}

\textbf{Continuità}

$f:\mathbb{R}\rightarrow \mathbb{R}$ continua

\begin{enumerate}[label=({\alph*})]

\item $X_n\overset {q.o.}\rightarrow X \Rightarrow f(X_x) \overset {q.o.}\rightarrow f(X)$

\item $X_n\overset {\mathbb{P}}\rightarrow X \Rightarrow f(X_x) \overset {\mathbb{P}}\rightarrow f(X)$

\end{enumerate}

\textbf{Legge debole dei grandi numeri (convergenza in probabilità)} $(X_n)_{n\geq 1}$ successione di v.a. i.i.d. t.c. $X_n\in L^2(\Omega,\mathcal{F},\mathbb{P})$ $\forall n\geq 1$, $\mu=\mathbb{E}[X_n]$, $\overline{X}_n=\frac{1}{n}\sum_{i=1}^nX_i$ (\textit{media campionaria}) $\Rightarrow\overline {X}_N\overset {\mathbb{P}}\rightarrow\mu$

\textbf{Legge forte dei grandi numeri} Nelle stesse ipotesi precedenti $\overline {X}_n \rightarrow\mu$ q.o. e in $L^2$

\textbf{Legge forte dei grandi numeri di Kolmogorov} $(X_n)_{n\geq 1}$ successione di v.a. i.i.d., $X_n\in L^1(\Omega,\mathcal{F},\mathbb{P})$, 

$\overline {X}_n \rightarrow\mu$ q.o. e in $L^1$

\textbf{Convergenza in legge} 

\begin{itemize}

\item $(\mu_n)_{n\in \mathbb{N}}$ e $\mu$ misure finite su $(\mathbb{R},\mathcal{B}(\mathbb{R}))$, la successione $(\mu_n)_{n\in \mathbb{N}}$ \textit{converge debolmente} a $\mu$ se $\forall f \in C_b(\mathbb{R})$ (continua e limitata) $\int_{\mathbb{R}} f\,d\mu_n\underset {n \rightarrow\infty}\rightarrow\int_{\mathbb{R}}f\,d\mu$ \quad $\mu_n\overset w \rightarrow \mu$ , $\mu_n\rightharpoonup\mu$

\item $(X_n)_{n\geq 1}$ e $X$ v.a. reali definite su $(\Omega_n,\mathcal{F}_n,\mathbb{P}_n)_{n\geq 1}$ e $(\Omega,\mathcal{F},\mathbb{P})$, la successione $(X_n)_{n\geq 1}$ \textit{converge in legge (in distribuzione)} a $X$ se la successione delle leggi $(\mathbb{P}_{X_n})_{n\geq 1}$ converge debolmente a $\mathbb{P}_X$, ovvero $\forall f \in C_b(\mathbb{R})$ $\int_{\mathbb{R}}f\,d \mathbb{P}_{X_n}\underset{n \rightarrow\infty}{\rightarrow}\int_{\mathbb{R}}f\,d \mathbb{P}_X \Leftrightarrow \int_{\Omega_n}f(X_n)\,d \mathbb{P}_n\underset{n \rightarrow\infty}{\rightarrow}\int_{\Omega}f(X)\,d \mathbb{P} \Leftrightarrow \mathbb{E}_n[f(X_n)]\underset{n \rightarrow\infty}{\rightarrow}\mathbb{E}[f(X)]$ \quad $X_n \overset{L}{\rightarrow}X$

Non è necessario che tutte le v.a. siano definite su un unico spazio di probabilità, quindi non si può paragonare con le altre convergenze, MA se le v.a. sono tutte definite sullo stesso spazio allora è la convergenza più debole

\item $(X_n)_{n\geq 1}$ e $X$ v.a. definite su $(\Omega,\mathcal{F},\mathbb{P})$ a valori in $\mathbb{R}$, $X_n \overset{\mathbb{P}}{\rightarrow}X\Rightarrow X_n \overset{L}{\rightarrow}X$

\item $(X_n)_{n\geq 1}$ e $X$ v.a. definite su $(\Omega,\mathcal{F},\mathbb{P})$ a valori in $\mathbb{R}$, $X=a$ q.o. (N.B. più restrittivo), $X_n \overset{L}{\rightarrow}X\Rightarrow X_n \overset{\mathbb{P}}{\rightarrow}X$

\end{itemize}

\textbf{Criteri di convergenza in legge}

\begin{enumerate}

\item $X_n \overset{L}{\rightarrow}X \Leftrightarrow F_n(x)\underset{n \rightarrow\infty}{\rightarrow}F(x)$ $\forall x$ punto di continuità di $F$ \textit{funzione di ripartizione} (N.B. nei punti di discontinuità di $F$ può non esserci la convergenza)

\item \underline{Teorema di Paul Lévy} 

\begin{enumerate}

\item $X_n \overset{L}{\rightarrow}X \Rightarrow \Phi_n(t)\rightarrow\Phi(t)\,\forall t\in \mathbb{R}$

\item $\Phi_n(t)\rightarrow\phi(t)\,\forall t\in \mathbb{R}$ e $\phi$ è continua in 0 allora $\exists X$ v.a. reale con \textit{funzione caratteristica} $\Phi(t)=\phi(t)$ e $X_n \overset{L}{\rightarrow}X$

\end{enumerate}

\end{enumerate}

\textbf{Proprietà convergenza in legge}

\begin{enumerate}

\item $f:\mathbb{R}\rightarrow \mathbb{R}$ continua $X_n \overset{L}{\rightarrow}X \Rightarrow f(X_n) \overset{L}{\rightarrow}f(X)$

\item $\left\{\begin{array}{ll}X_n \rightarrow X & q.o.\,(L^p,\mathbb{P})\\ Y_n \rightarrow Y & q.o.\,(L^p,\mathbb{P})\end{array} \right. \Rightarrow X_n+Y_n \rightarrow X+Y\,q.o.\,(L^p,\mathbb{P})$ MA non è vero in generale per la convergenza in legge:

\begin{enumerate}

\item $\left\{\begin{array}{ll}X_n \overset{L}\rightarrow X & \text{ma anche }q.o.\,(L^p,\mathbb{P})\\ Y_n\overset{L}\rightarrow Y & \text{ma anche }q.o.\,(L^p,\mathbb{P})\end{array} \right. \Rightarrow X_n+Y_n \rightarrow X+Y\,q.o.\,(L^p,\mathbb{P})$ e quindi in legge

\item $X_n \overset{L}\rightarrow X\quad Y_n \overset{L}\rightarrow Y\quad X_n\ind Y_n\,\forall n\in \mathbb{N}\quad X\ind Y \Rightarrow X_n+Y_n\overset{L}{\rightarrow}X+Y$

\item \underline{Teorema di Slutsky} $X_n \overset{L}\rightarrow X\quad Y_n \overset{L}\rightarrow a\, (\Leftrightarrow Y_n \overset{\mathbb{P}}{\rightarrow}a)$ , allora:

\begin{enumerate}

\item $X_n+Y_n \overset{L}{\rightarrow}X+a$

\item $X_nY_n \overset{L}{\rightarrow}Xa$

\item $\frac{X_n}{Y_n} \overset{L}{\rightarrow}\frac{X}{a}$ (se ben definite)

\end{enumerate}

\end{enumerate}

\end{enumerate}

\textbf{Teorema limite centrale} $(X_n)_{n\in \mathbb{N}}$ successione di v.a. reali, i.i.d e $X_n\in L^2(\Omega,\mathcal{F},\mathbb{P})\,\forall n\geq 1$ , $\mu=\mathbb{E}[X_n]$, $\sigma^2=\text{Var}(X_n)$ , $S_n=\sum_{i=1}^nX_i$ e $S_n^*=\frac{S_n-n\mu}{\sqrt{n\sigma^2}}$ la sua \textit{standardizzazione} $\Rightarrow S_n^*\overset{L}{\rightarrow}N$ con $N\sim \mathcal{N}(0,1)$

\begin {wraptable}{l}{8cm}

\begin{tabular}{c|c}

\textbf{Osservazioni} & Per $n$ grande \\

\hline

$S_n^*=\frac{S_n-n\mu}{\sqrt{n\sigma^2}}\overset{L}{\rightarrow}\mathcal{N}(0,1)$ & $S_n^*\approx \mathcal{N}(0,1)$ \\

$\frac{1}{\sqrt{n}}(S_n-n\mu)\overset{L}{\rightarrow}\mathcal{N}(0,\sigma^2)$ & $S_n\approx \mathcal{N}(n\mu,n\sigma^2)$ \\

$\sqrt{n}(\overline X_n-\mu)\overset{L}{\rightarrow}\mathcal{N}(0,\sigma^2)$ & $\overline X_n\approx \mathcal{N}(\mu,\frac{\sigma^2}{n})$ \\

\end{tabular}

\end {wraptable}

\hfill

\hfill

\underline{\textbf{NON FARE}}

$S_n\overset{L}{\rightarrow}\mathcal{N}(n\mu,n\sigma^2)$

$\overline X_n \overset{L}{\rightarrow}\mathcal{N}(\mu,\frac{\sigma^2}{n})$

il limite $n \rightarrow \infty$ deve comparire solo a sinistra


\end{document}

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  